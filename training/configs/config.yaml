model:
  name: "mistralai/Mistral-7B-Instruct-v0.3"
  type: "mistral"
  max_length: 2048
  tokenizer_kwargs:
    trust_remote_code: true
  model_kwargs:
    trust_remote_code: true
    dtype: "bfloat16"

dataset:
  path: "data/conversations.json"
  tokenized_examples_path: "data/tokenized_examples_2048.pt"
  train_split: 0.90
  val_split: 0.05
  test_split: 0.05
  random_seed: 42
  padding_side: "left"

training:
  epochs: 2
  batch_size: 16
  gradient_accumulation_steps: 4
  learning_rate: 2e-5
  eval_steps: 2000
  save_steps: 1000
  gradient_checkpointing: true
  dataloader_workers: 4
  periodic_checkpoint_interval: 1800  # Periodic checkpoint interval in seconds (0 to disable). 1800 = 30 minutes

wandb:
  project: "yap-for-me"
  entity: "ryanamiri05-northeastern-university"