{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Playground\n",
        "\n",
        "This notebook lets you:\n",
        "- Download and load checkpoints from training runs\n",
        "- Test the model with conversation history\n",
        "- See full reasoning and tool calls\n",
        "- Simulate text messages and actions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "import json\n",
        "import re\n",
        "import random\n",
        "\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import yaml\n",
        "\n",
        "# Add training directory to path (notebook is in training/notebooks/)\n",
        "notebook_dir = Path.cwd()\n",
        "training_dir = notebook_dir.parent\n",
        "sys.path.insert(0, str(training_dir))\n",
        "\n",
        "from dataloader import format_message\n",
        "from config import Config\n",
        "\n",
        "# Workaround: some transformers versions fail when config contains torch.dtype (not JSON-serializable in logger)\n",
        "import transformers.configuration_utils as _conf_utils\n",
        "_orig_to_dict = _conf_utils.PretrainedConfig.to_dict\n",
        "def _to_dict_dtype_safe(self):\n",
        "    d = _orig_to_dict(self)\n",
        "    for k, v in list(d.items()):\n",
        "        if isinstance(v, torch.dtype):\n",
        "            d[k] = str(v)\n",
        "        elif hasattr(v, \"__class__\") and v.__class__.__name__ == \"dtype\":\n",
        "            d[k] = str(v)\n",
        "    return d\n",
        "_conf_utils.PretrainedConfig.to_dict = _to_dict_dtype_safe\n",
        "\n",
        "# Device detection for Mac (MPS), CUDA, or CPU\n",
        "def get_device():\n",
        "    \"\"\"Get the best available device for inference.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return \"cuda\"\n",
        "    elif torch.backends.mps.is_available():\n",
        "        return \"mps\"\n",
        "    else:\n",
        "        return \"cpu\"\n",
        "\n",
        "# Set default device\n",
        "DEVICE = get_device()\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "if DEVICE == \"mps\":\n",
        "    print(\"Note: MPS (Metal) acceleration enabled for Mac GPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_latest_checkpoint_remote(job_id: str, remote_path: str = \"/projects/llpr/amiri.ry/projects/yap-for-me/training\") -> Optional[str]:\n",
        "    \"\"\"Find the latest checkpoint in a remote job directory.\n",
        "    \n",
        "    Args:\n",
        "        job_id: SLURM job ID\n",
        "        remote_path: Remote path to training directory\n",
        "    \n",
        "    Returns:\n",
        "        Path to latest checkpoint directory, or None if not found\n",
        "    \"\"\"\n",
        "    remote_job_dir = f\"{remote_path}/out/{job_id}\"\n",
        "    \n",
        "    print(f\"Checking remote directory: {remote_job_dir}\")\n",
        "    \n",
        "    # List all subdirectories in the job output directory\n",
        "    cmd = f\"ssh amiri.ry@login.explorer.northeastern.edu 'ls -d {remote_job_dir}/*/ 2>/dev/null | head -20'\"\n",
        "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "    \n",
        "    if result.returncode != 0:\n",
        "        print(f\"Error listing directories. Return code: {result.returncode}\")\n",
        "        print(f\"stderr: {result.stderr}\")\n",
        "        print(f\"stdout: {result.stdout}\")\n",
        "        return None\n",
        "    \n",
        "    checkpoint_dirs = [line.strip().rstrip('/') for line in result.stdout.strip().split('\\n') if line.strip()]\n",
        "    \n",
        "    print(f\"Found {len(checkpoint_dirs)} directories: {checkpoint_dirs}\")\n",
        "    \n",
        "    if not checkpoint_dirs:\n",
        "        print(f\"No directories found in {remote_job_dir}\")\n",
        "        return None\n",
        "    \n",
        "    # Check which ones have training_state.pt and get their step numbers\n",
        "    checkpoints_with_steps = []\n",
        "    total = len(checkpoint_dirs)\n",
        "    \n",
        "    for idx, checkpoint_dir in enumerate(checkpoint_dirs, 1):\n",
        "        state_file = f\"{checkpoint_dir}/training_state.pt\"\n",
        "        checkpoint_name = Path(checkpoint_dir).name\n",
        "        \n",
        "        print(f\"\\n[{idx}/{total}] Checking {checkpoint_name}...\")\n",
        "        \n",
        "        # First check if file exists\n",
        "        print(f\"  Checking if {state_file} exists...\")\n",
        "        check_cmd = f\"ssh amiri.ry@login.explorer.northeastern.edu 'test -f {state_file} && echo exists || echo notfound'\"\n",
        "        check_result = subprocess.run(check_cmd, shell=True, capture_output=True, text=True)\n",
        "        \n",
        "        if check_result.stdout.strip() != \"exists\":\n",
        "            print(f\"  {checkpoint_name}: No training_state.pt (skipping)\")\n",
        "            continue\n",
        "        \n",
        "        # Try to parse step number from checkpoint name (e.g., \"checkpoint-2000\" -> 2000)\n",
        "        step = None\n",
        "        if checkpoint_name.startswith(\"checkpoint-\"):\n",
        "            try:\n",
        "                step = int(checkpoint_name.split(\"-\")[-1])\n",
        "                print(f\"  ✓ {checkpoint_name}: step {step} (from name)\")\n",
        "            except ValueError:\n",
        "                pass\n",
        "        \n",
        "        # If we couldn't parse from name, read from training_state.pt\n",
        "        if step is None:\n",
        "            print(f\"  {checkpoint_name}: Reading step number from training_state.pt...\")\n",
        "            # Read step number directly on remote server\n",
        "            escaped_file = state_file.replace('\"', '\\\\\"')\n",
        "            python_cmd = f\"python3 -c 'import torch; state = torch.load(\\\"{escaped_file}\\\", map_location=\\\"cpu\\\", weights_only=False); print(state.get(\\\"step\\\", 0))'\"\n",
        "            escaped_python = python_cmd.replace('\"', '\\\\\"')\n",
        "            remote_cmd = f'ssh amiri.ry@login.explorer.northeastern.edu \"{escaped_python}\"'\n",
        "            \n",
        "            try:\n",
        "                result = subprocess.run(remote_cmd, shell=True, capture_output=True, text=True, timeout=30)\n",
        "                \n",
        "                if result.returncode == 0:\n",
        "                    try:\n",
        "                        step = int(result.stdout.strip())\n",
        "                        print(f\"  ✓ {checkpoint_name}: step {step} (from training_state.pt)\")\n",
        "                    except ValueError:\n",
        "                        print(f\"  ✗ {checkpoint_name}: Could not parse step number: {result.stdout[:100]}\")\n",
        "                        continue\n",
        "                else:\n",
        "                    print(f\"  ✗ {checkpoint_name}: Error reading step remotely\")\n",
        "                    print(f\"    stderr: {result.stderr[:200]}\")\n",
        "                    continue\n",
        "            except subprocess.TimeoutExpired:\n",
        "                print(f\"  ✗ {checkpoint_name}: Timeout reading step (took >30s)\")\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"  ✗ {checkpoint_name}: Error reading step: {e}\")\n",
        "                continue\n",
        "        \n",
        "        checkpoints_with_steps.append((checkpoint_dir, step))\n",
        "    \n",
        "    if not checkpoints_with_steps:\n",
        "        print(f\"\\nNo valid checkpoints found with training_state.pt\")\n",
        "        return None\n",
        "    \n",
        "    # Sort by step and return the latest\n",
        "    checkpoints_with_steps.sort(key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(f\"Summary: Found {len(checkpoints_with_steps)} checkpoint(s):\")\n",
        "    for checkpoint_dir, step in checkpoints_with_steps:\n",
        "        checkpoint_name = Path(checkpoint_dir).name\n",
        "        print(f\"  - {checkpoint_name}: step {step}\")\n",
        "    print(f\"=\"*60)\n",
        "    \n",
        "    latest_checkpoint = checkpoints_with_steps[0][0]\n",
        "    latest_step = checkpoints_with_steps[0][1]\n",
        "    latest_name = Path(latest_checkpoint).name\n",
        "    print(f\"\\nUsing latest checkpoint: {latest_name} (step {latest_step})\")\n",
        "    return latest_checkpoint\n",
        "\n",
        "def download_checkpoint(job_id: str, local_dir: Optional[str] = None, remote_path: str = \"/projects/llpr/amiri.ry/projects/yap-for-me/training\") -> Optional[str]:\n",
        "    \"\"\"Download the latest checkpoint from a training run using rsync.\n",
        "    \n",
        "    Args:\n",
        "        job_id: SLURM job ID\n",
        "        local_dir: Local directory to save checkpoint (defaults to training/checkpoints)\n",
        "        remote_path: Remote path to training directory\n",
        "    \n",
        "    Returns:\n",
        "        Path to local checkpoint directory, or None if failed\n",
        "    \"\"\"\n",
        "    print(f\"Finding latest checkpoint for job {job_id}...\")\n",
        "    remote_checkpoint = find_latest_checkpoint_remote(job_id, remote_path)\n",
        "    \n",
        "    if not remote_checkpoint:\n",
        "        return None\n",
        "    \n",
        "    # Default to training/checkpoints (parent of notebooks directory)\n",
        "    if local_dir is None:\n",
        "        local_dir = str(training_dir / \"checkpoints\")\n",
        "    \n",
        "    # Create local directory path\n",
        "    local_checkpoint_dir = Path(local_dir) / job_id / Path(remote_checkpoint).name\n",
        "    local_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # IMPORTANT: Use the dedicated transfer node (xfer.discovery.neu.edu) NOT login node!\n",
        "    # According to Northeastern Explorer HPC documentation:\n",
        "    # - Transfer node: xfer.discovery.neu.edu (optimized for data transfers)\n",
        "    # - Login node: login.explorer.northeastern.edu (NOT for transfers)\n",
        "    transfer_node = \"xfer.discovery.neu.edu\"\n",
        "    username = \"amiri.ry\"\n",
        "    \n",
        "    local_full_path = str(local_checkpoint_dir.absolute())\n",
        "    \n",
        "    # Rsync via transfer node (recommended by Northeastern)\n",
        "    rsync_cmd = f\"rsync -av --partial --progress {username}@{transfer_node}:{remote_checkpoint}/ {local_full_path}/\"\n",
        "    \n",
        "    print(f\"\\nDownloading checkpoint from {remote_checkpoint}...\")\n",
        "    print(f\"Using transfer node: {transfer_node}\")\n",
        "    print(f\"Local destination: {local_full_path}\")\n",
        "    print(f\"\\nRunning: {rsync_cmd}\")\n",
        "    print(f\"This may take several minutes for large checkpoints...\\n\")\n",
        "    \n",
        "    # Execute rsync command\n",
        "    result = subprocess.run(rsync_cmd, shell=True)\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        print(f\"\\n✓ Checkpoint downloaded successfully to {local_full_path}\")\n",
        "        return str(local_full_path)\n",
        "    else:\n",
        "        print(f\"\\n✗ Download failed (return code: {result.returncode})\")\n",
        "        print(f\"\\nAlternative methods:\")\n",
        "        print(f\"1. Try running the command manually:\")\n",
        "        print(f\"   {rsync_cmd}\")\n",
        "        print(f\"2. Use Globus (recommended for large files):\")\n",
        "        print(f\"   - Visit: https://www.globus.org/\")\n",
        "        print(f\"   - Sign in with Northeastern credentials\")\n",
        "        print(f\"   - Install Globus Connect Personal\")\n",
        "        print(f\"   - Search for 'Discovery Cluster' endpoint\")\n",
        "        print(f\"   - Transfer files via web interface\")\n",
        "        return None\n",
        "\n",
        "def load_model_from_checkpoint(checkpoint_path: str, device: Optional[str] = None):\n",
        "    \"\"\"Load model and tokenizer from checkpoint.\n",
        "    \n",
        "    Args:\n",
        "        checkpoint_path: Path to checkpoint directory\n",
        "        device: Device to load model on (defaults to auto-detected best device)\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (model, tokenizer)\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = get_device()\n",
        "    \n",
        "    checkpoint_path = Path(checkpoint_path)\n",
        "    \n",
        "    print(f\"Loading tokenizer from {checkpoint_path}...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path, trust_remote_code=True)\n",
        "    \n",
        "    print(f\"Loading model from {checkpoint_path}...\")\n",
        "    \n",
        "    # For MPS, we might need to use float16 instead of bfloat16\n",
        "    if device == \"mps\":\n",
        "        # MPS supports float16 but bfloat16 support may vary\n",
        "        try:\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                checkpoint_path,\n",
        "                dtype=torch.float16,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: float16 failed, trying bfloat16: {e}\")\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                checkpoint_path,\n",
        "                dtype=torch.bfloat16,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "    else:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            checkpoint_path,\n",
        "            dtype=torch.bfloat16,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "    \n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    print(f\"Model loaded on {device}\")\n",
        "    return model, tokenizer\n",
        "\n",
        "def format_conversation_history(messages: List[Dict], max_context_tokens: int = 1800) -> str:\n",
        "    \"\"\"Format conversation history for model input.\n",
        "    \n",
        "    Args:\n",
        "        messages: List of message dicts with keys: timestamp, speaker, text, replying_to, guid\n",
        "        max_context_tokens: Maximum tokens for context (leaves room for tool calls)\n",
        "    \n",
        "    Returns:\n",
        "        Formatted conversation string\n",
        "    \"\"\"\n",
        "    formatted_messages = []\n",
        "    for msg in messages:\n",
        "        formatted = format_message(\n",
        "            msg.get('timestamp', ''),\n",
        "            msg.get('speaker', 'Unknown'),\n",
        "            msg.get('text', ''),\n",
        "            msg.get('replying_to'),\n",
        "            msg.get('guid')\n",
        "        )\n",
        "        formatted_messages.append(formatted + '\\n')\n",
        "    \n",
        "    return ''.join(formatted_messages)\n",
        "\n",
        "def generate_tool_calls(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    conversation_history: str,\n",
        "    max_new_tokens: int = 512,\n",
        "    temperature: float = 0.7,\n",
        "    top_p: float = 0.9,\n",
        "    device: Optional[str] = None\n",
        ") -> str:\n",
        "    \"\"\"Generate tool calls given conversation history.\n",
        "    \n",
        "    Args:\n",
        "        model: Loaded model\n",
        "        tokenizer: Loaded tokenizer\n",
        "        conversation_history: Formatted conversation history\n",
        "        max_new_tokens: Maximum tokens to generate\n",
        "        temperature: Sampling temperature\n",
        "        top_p: Nucleus sampling parameter\n",
        "        device: Device to run inference on (defaults to auto-detected best device)\n",
        "    \n",
        "    Returns:\n",
        "        Generated tool calls text\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = get_device()\n",
        "    \n",
        "    # Tokenize the conversation history\n",
        "    inputs = tokenizer(\n",
        "        conversation_history,\n",
        "        return_tensors=\"pt\",\n",
        "        add_special_tokens=False\n",
        "    ).to(device)\n",
        "    \n",
        "    # Generate tool calls\n",
        "    with torch.no_grad():\n",
        "        # MPS may have issues with some generation parameters, so we handle it carefully\n",
        "        generation_kwargs = {\n",
        "            \"max_new_tokens\": max_new_tokens,\n",
        "            \"temperature\": temperature,\n",
        "            \"top_p\": top_p,\n",
        "            \"do_sample\": True,\n",
        "            \"pad_token_id\": tokenizer.eos_token_id\n",
        "        }\n",
        "        \n",
        "        # MPS sometimes has issues with certain generation settings\n",
        "        if device == \"mps\":\n",
        "            # Ensure we're using compatible settings for MPS\n",
        "            try:\n",
        "                outputs = model.generate(**inputs, **generation_kwargs)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Generation with MPS failed, trying with adjusted settings: {e}\")\n",
        "                # Fallback: disable some optimizations that might not work on MPS\n",
        "                generation_kwargs.pop(\"top_p\", None)\n",
        "                outputs = model.generate(**inputs, **generation_kwargs)\n",
        "        else:\n",
        "            outputs = model.generate(**inputs, **generation_kwargs)\n",
        "    \n",
        "    # Decode only the newly generated tokens\n",
        "    input_length = inputs['input_ids'].shape[1]\n",
        "    generated_tokens = outputs[0][input_length:]\n",
        "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "    \n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage Example\n",
        "\n",
        "### Step 1: Download and load a checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace with your job ID\n",
        "JOB_ID = \"3105120\"\n",
        "\n",
        "# Download the latest checkpoint\n",
        "# checkpoint_path = download_checkpoint(JOB_ID)\n",
        "checkpoint_path = \"/Users/ramiri/dev/projects/YapForMe/training/checkpoints/3105120/checkpoint-2000\"\n",
        "\n",
        "if checkpoint_path:\n",
        "    # Load the model\n",
        "    model, tokenizer = load_model_from_checkpoint(checkpoint_path)\n",
        "    print(\"Model ready for inference!\")\n",
        "else:\n",
        "    print(\"Failed to download checkpoint\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Create conversation history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example conversation history\n",
        "conversation_messages = [\n",
        "    {\n",
        "        \"timestamp\": \"2025-12-09 10:00:00\",\n",
        "        \"speaker\": \"Alice\",\n",
        "        \"text\": \"Hey, are you free for lunch today?\",\n",
        "        \"replying_to\": None,\n",
        "        \"guid\": \"1\"\n",
        "    },\n",
        "    {\n",
        "        \"timestamp\": \"2025-12-09 10:05:00\",\n",
        "        \"speaker\": \"Ryan\",\n",
        "        \"text\": \"Let me check my calendar\",\n",
        "        \"replying_to\": None,\n",
        "        \"guid\": \"2\"\n",
        "    },\n",
        "    {\n",
        "        \"timestamp\": \"2025-12-09 10:06:00\",\n",
        "        \"speaker\": \"Alice\",\n",
        "        \"text\": \"Sounds good! Let me know\",\n",
        "        \"replying_to\": None,\n",
        "        \"guid\": \"3\"\n",
        "    },\n",
        "]\n",
        "\n",
        "# Format the conversation\n",
        "formatted_history = format_conversation_history(conversation_messages)\n",
        "print(\"Conversation History:\")\n",
        "print(\"=\" * 60)\n",
        "print(formatted_history)\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Generate tool calls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate tool calls based on conversation history\n",
        "tool_calls = generate_tool_calls(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    formatted_history,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9\n",
        ")\n",
        "\n",
        "print(\"Generated Tool Calls:\")\n",
        "print(\"=\" * 60)\n",
        "print(tool_calls)\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nFull Input + Output Sequence:\")\n",
        "print(\"=\" * 60)\n",
        "full_sequence = formatted_history + tool_calls\n",
        "print(full_sequence)\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Try to extract and display individual tool calls if they're in a structured format\n",
        "# Look for common tool call patterns (adjust based on your actual format)\n",
        "tool_patterns = [\n",
        "    r'<tool_call>(.*?)</tool_call>',\n",
        "    r'```(?:json|python)?\\s*(\\{.*?\\})\\s*```',\n",
        "    r'(\\w+)\\s*\\([^)]*\\)',\n",
        "]\n",
        "\n",
        "print(\"\\nExtracted Tools:\")\n",
        "print(\"=\" * 60)\n",
        "tools_found = []\n",
        "for pattern in tool_patterns:\n",
        "    matches = re.findall(pattern, tool_calls, re.DOTALL | re.IGNORECASE)\n",
        "    if matches:\n",
        "        tools_found.extend(matches)\n",
        "        break\n",
        "\n",
        "if tools_found:\n",
        "    for i, tool in enumerate(tools_found, 1):\n",
        "        print(f\"Tool {i}:\")\n",
        "        print(tool)\n",
        "        print()\n",
        "else:\n",
        "    print(\"No structured tool calls detected. Raw output shown above.\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interactive Testing\n",
        "\n",
        "You can build conversations interactively and test the model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_conversation(messages: List[Dict], model, tokenizer):\n",
        "    \"\"\"Simulate a conversation and see what the model would respond with.\"\"\"\n",
        "    formatted_history = format_conversation_history(messages)\n",
        "    \n",
        "    print(\"Current Conversation:\")\n",
        "    print(\"=\" * 60)\n",
        "    for msg in messages:\n",
        "        print(f\"[{msg.get('timestamp', '')}] {msg.get('speaker', '')}: {msg.get('text', '')}\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\n\")\n",
        "    \n",
        "    tool_calls = generate_tool_calls(model, tokenizer, formatted_history)\n",
        "    \n",
        "    print(\"Model's Response (Tool Calls):\")\n",
        "    print(\"=\" * 60)\n",
        "    print(tool_calls)\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return tool_calls\n",
        "\n",
        "# Example usage\n",
        "test_messages = [\n",
        "    {\"timestamp\": \"2025-12-09 10:00:00\", \"speaker\": \"Friend\", \"text\": \"What are you up to?\", \"replying_to\": None, \"guid\": None},\n",
        "]\n",
        "\n",
        "# Uncomment to test:\n",
        "# simulate_conversation(test_messages, model, tokenizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load from existing local checkpoint\n",
        "\n",
        "If you already have a checkpoint downloaded, you can load it directly:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load from an existing local checkpoint\n",
        "# checkpoint_path = str(training_dir / \"checkpoints\" / \"3105120\" / \"checkpoint-2000\")\n",
        "# model, tokenizer = load_model_from_checkpoint(checkpoint_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment with generation parameters\n",
        "\n",
        "Try different temperature and top_p values to see how the model responds:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try different temperatures\n",
        "# Lower temperature = more focused/deterministic\n",
        "# Higher temperature = more creative/random\n",
        "\n",
        "test_temps = [0.3, 0.7, 1.0]\n",
        "test_context = format_conversation_history([\n",
        "    {\"timestamp\": \"2025-12-09 14:00:00\", \"speaker\": \"Mom\", \"text\": \"Are you coming home for dinner?\", \"replying_to\": None, \"guid\": None},\n",
        "])\n",
        "\n",
        "for temp in test_temps:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Temperature: {temp}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    response = generate_tool_calls(\n",
        "        model, tokenizer, test_context,\n",
        "        temperature=temp,\n",
        "        max_new_tokens=256\n",
        "    )\n",
        "    print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Chat\n",
        "\n",
        "Chat with the model in real-time. You type messages as \"the other person\" and the model responds as Ryan.\n",
        "\n",
        "Run this cell and follow the prompts. Type \"quit\" or \"exit\" to stop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_tool_calls(model_output: str) -> list:\n",
        "    \"\"\"Parse code-style tool calls from model output.\n",
        "    \n",
        "    Extracts tool calls in format:\n",
        "    - react(message_guid=\"...\", reaction_type=\"...\")\n",
        "    - reply(message_guid=\"...\", text=\"...\")\n",
        "    - send_message(text=\"...\")\n",
        "    \n",
        "    Returns list of dicts with 'action_type' and 'params' keys.\n",
        "    \"\"\"\n",
        "    tool_calls = []\n",
        "    \n",
        "    lines = model_output.strip().split('\\n')\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        \n",
        "        react_match = re.match(r'react\\s*\\(\\s*message_guid\\s*=\\s*\"([^\"]+)\"\\s*,\\s*reaction_type\\s*=\\s*\"([^\"]+)\"\\s*\\)', line)\n",
        "        if react_match:\n",
        "            tool_calls.append({\n",
        "                'action_type': 'react',\n",
        "                'params': {\n",
        "                    'message_guid': react_match.group(1),\n",
        "                    'reaction_type': react_match.group(2)\n",
        "                }\n",
        "            })\n",
        "            continue\n",
        "        \n",
        "        reply_match = re.match(r'reply\\s*\\(\\s*message_guid\\s*=\\s*\"([^\"]+)\"\\s*,\\s*text\\s*=\\s*(.+?)\\s*\\)', line, re.DOTALL)\n",
        "        if reply_match:\n",
        "            text_value = reply_match.group(2).strip()\n",
        "            if text_value.startswith('\"') and text_value.endswith('\"'):\n",
        "                text_value = text_value[1:-1]\n",
        "            elif text_value.startswith(\"'\") and text_value.endswith(\"'\"):\n",
        "                text_value = text_value[1:-1]\n",
        "            tool_calls.append({\n",
        "                'action_type': 'reply',\n",
        "                'params': {\n",
        "                    'message_guid': reply_match.group(1),\n",
        "                    'text': text_value\n",
        "                }\n",
        "            })\n",
        "            continue\n",
        "        \n",
        "        send_msg_match = re.match(r'send_message\\s*\\(\\s*text\\s*=\\s*(.+?)\\s*\\)', line, re.DOTALL)\n",
        "        if send_msg_match:\n",
        "            text_value = send_msg_match.group(1).strip()\n",
        "            if text_value.startswith('\"') and text_value.endswith('\"'):\n",
        "                text_value = text_value[1:-1]\n",
        "            elif text_value.startswith(\"'\") and text_value.endswith(\"'\"):\n",
        "                text_value = text_value[1:-1]\n",
        "            tool_calls.append({\n",
        "                'action_type': 'send_message',\n",
        "                'params': {\n",
        "                    'text': text_value\n",
        "                }\n",
        "            })\n",
        "            continue\n",
        "    \n",
        "    return tool_calls\n",
        "\n",
        "\n",
        "def format_tool_calls_display(tool_calls: list, conversation: list) -> str:\n",
        "    \"\"\"Format parsed tool calls for display.\n",
        "    \n",
        "    Returns a human-readable string showing what Ryan would do.\n",
        "    \"\"\"\n",
        "    if not tool_calls:\n",
        "        return \"(No valid tool calls parsed)\"\n",
        "    \n",
        "    outputs = []\n",
        "    for tc in tool_calls:\n",
        "        action = tc['action_type']\n",
        "        params = tc['params']\n",
        "        \n",
        "        if action == 'send_message':\n",
        "            outputs.append(params.get('text', ''))\n",
        "        elif action == 'reply':\n",
        "            target_guid = params.get('message_guid', '')\n",
        "            text = params.get('text', '')\n",
        "            target_msg = None\n",
        "            for msg in conversation:\n",
        "                if msg.get('guid') == target_guid:\n",
        "                    target_msg = msg\n",
        "                    break\n",
        "            if target_msg:\n",
        "                outputs.append(f\"[replying to \\\"{target_msg['text'][:30]}...\\\"] {text}\")\n",
        "            else:\n",
        "                outputs.append(f\"[reply] {text}\")\n",
        "        elif action == 'react':\n",
        "            reaction = params.get('reaction_type', '')\n",
        "            target_guid = params.get('message_guid', '')\n",
        "            target_msg = None\n",
        "            for msg in conversation:\n",
        "                if msg.get('guid') == target_guid:\n",
        "                    target_msg = msg\n",
        "                    break\n",
        "            if target_msg:\n",
        "                outputs.append(f\"[{reaction} reaction to \\\"{target_msg['text'][:30]}...\\\"]\")\n",
        "            else:\n",
        "                outputs.append(f\"[{reaction} reaction]\")\n",
        "    \n",
        "    return '\\n'.join(outputs) if outputs else \"(empty response)\"\n",
        "\n",
        "\n",
        "def interactive_chat(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    temperature: float = 0.7,\n",
        "    top_p: float = 0.9,\n",
        "    max_new_tokens: int = 128,\n",
        "    show_raw_output: bool = False\n",
        "):\n",
        "    \"\"\"Interactive chat loop with the model.\n",
        "    \n",
        "    You play the role of someone texting Ryan, and the model responds as Ryan.\n",
        "    Type 'quit' or 'exit' to stop.\n",
        "    \n",
        "    Args:\n",
        "        model: Loaded model\n",
        "        tokenizer: Loaded tokenizer\n",
        "        temperature: Sampling temperature (0.3-0.5 focused, 0.8-1.0 creative)\n",
        "        top_p: Nucleus sampling parameter\n",
        "        max_new_tokens: Max tokens to generate\n",
        "        show_raw_output: If True, also shows raw tool call output\n",
        "    \"\"\"\n",
        "    from datetime import datetime, timedelta\n",
        "    \n",
        "    print(\"Interactive Chat with Ryan\")\n",
        "    print(\"Type your messages and see how Ryan would respond.\")\n",
        "    print(\"Type 'quit' or 'exit' to stop.\\n\")\n",
        "    \n",
        "    partner_name = input(\"Who are you chatting as? (e.g., Mom, Alice, Friend): \").strip()\n",
        "    if not partner_name:\n",
        "        partner_name = \"Friend\"\n",
        "    \n",
        "    print(f\"\\nYou are now chatting as '{partner_name}'. Start the conversation!\\n\")\n",
        "    \n",
        "    conversation = []\n",
        "    current_time = datetime.now()\n",
        "    msg_id = 1\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(f\"{partner_name}: \").strip()\n",
        "        except (EOFError, KeyboardInterrupt):\n",
        "            print(\"\\n\\nChat ended.\")\n",
        "            break\n",
        "        \n",
        "        if user_input.lower() in (\"quit\", \"exit\", \"q\"):\n",
        "            print(\"\\nChat ended.\")\n",
        "            break\n",
        "        \n",
        "        if not user_input:\n",
        "            continue\n",
        "        \n",
        "        timestamp_str = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        conversation.append({\n",
        "            \"timestamp\": timestamp_str,\n",
        "            \"speaker\": partner_name,\n",
        "            \"text\": user_input,\n",
        "            \"replying_to\": None,\n",
        "            \"guid\": str(msg_id)\n",
        "        })\n",
        "        msg_id += 1\n",
        "        current_time += timedelta(seconds=random.randint(5, 30))\n",
        "        \n",
        "        formatted_history = format_conversation_history(conversation)\n",
        "        \n",
        "        print(\"\\nRyan is typing...\")\n",
        "        raw_response = generate_tool_calls(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            formatted_history,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p\n",
        "        )\n",
        "        \n",
        "        response_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        current_time += timedelta(seconds=random.randint(10, 60))\n",
        "        \n",
        "        tool_calls = parse_tool_calls(raw_response)\n",
        "        display_text = format_tool_calls_display(tool_calls, conversation)\n",
        "        \n",
        "        if show_raw_output:\n",
        "            print(f\"\\n[Raw output]: {raw_response}\")\n",
        "        print(f\"\\nRyan: {display_text}\\n\")\n",
        "        \n",
        "        for tc in tool_calls:\n",
        "            action = tc['action_type']\n",
        "            params = tc['params']\n",
        "            \n",
        "            if action in ('send_message', 'reply'):\n",
        "                text = params.get('text', raw_response)\n",
        "                conversation.append({\n",
        "                    \"timestamp\": response_time,\n",
        "                    \"speaker\": \"Ryan\",\n",
        "                    \"text\": text,\n",
        "                    \"replying_to\": None,\n",
        "                    \"guid\": str(msg_id)\n",
        "                })\n",
        "                msg_id += 1\n",
        "                current_time += timedelta(seconds=random.randint(2, 10))\n",
        "                response_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        \n",
        "        if not tool_calls:\n",
        "            conversation.append({\n",
        "                \"timestamp\": response_time,\n",
        "                \"speaker\": \"Ryan\",\n",
        "                \"text\": raw_response,\n",
        "                \"replying_to\": None,\n",
        "                \"guid\": str(msg_id)\n",
        "            })\n",
        "            msg_id += 1\n",
        "    \n",
        "    if conversation:\n",
        "        print(\"\\nFull conversation:\")\n",
        "        print(\"-\" * 60)\n",
        "        for msg in conversation:\n",
        "            print(f\"[{msg['timestamp']}] {msg['speaker']}: {msg['text']}\")\n",
        "        print(\"-\" * 60)\n",
        "    \n",
        "    return conversation\n",
        "\n",
        "# Start the interactive chat (make sure model and tokenizer are loaded first)\n",
        "# Adjust temperature for different response styles:\n",
        "#   - Lower (0.3-0.5): More focused, predictable\n",
        "#   - Higher (0.8-1.0): More creative, varied\n",
        "# Set show_raw_output=True to see the raw tool calls\n",
        "chat_history = interactive_chat(model, tokenizer, temperature=0.7, show_raw_output=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
